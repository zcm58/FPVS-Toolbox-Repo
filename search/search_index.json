{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"FPVS Toolbox Documentation","text":"<p>The Fast Periodic Visual Stimulation (FPVS) Toolbox allows you to easily process EEG data from FPVS experiments and run statistical analyses on the resulting metrics. </p> <p>Current app version: v1.5.0</p>"},{"location":"#quick-start","title":"Quick start","text":"<ol> <li> <p>Install FPVS Toolbox    Download the latest installer from the GitHub Releases page and run it    on Windows. You may have to bypass Windows Defender if prompted. </p> </li> <li> <p>Create or open a project    From the main window, choose a project root folder and set basic metadata    (project name, groups, subject list, trigger IDs).</p> </li> <li> <p>Process EEG data </p> </li> <li>Select an EEG file or batch folder.  </li> <li>Configure preprocessing (reference, downsampling, filters, artifact      handling) in the Settings panel.  </li> <li> <p>Click Start Processing to run the pipeline and generate metrics      and Excel outputs.</p> </li> <li> <p>Run statistical analysis</p> </li> <li> <p>Open Statistical Analysis in the sidebar to run single-group or    between-group analyses on the processed data.</p> </li> </ol>"},{"location":"#core-documentation","title":"Core documentation","text":"<ul> <li> <p>Processing Pipeline   Detailed description of how recordings are loaded, preprocessed, epoched,   and converted to frequency-domain metrics.</p> </li> <li> <p>Statistical Analysis   Overview of single-group and between-group analyses, models used, and   how to interpret the outputs.</p> </li> <li> <p>Relevant Publications   A brief overview of relevant publications on FPVS and the preprocessing pipeline used inside the FPVS Toolbox. </p> </li> <li> <p>Tutorials &amp; Walkthroughs   Step-by-step examples on how to use the FPVS Toolbox.</p> </li> </ul>"},{"location":"#project-links","title":"Project links","text":"<ul> <li>GitHub repository: <code>https://github.com/zcm58/FPVS-Toolbox-Repo&gt;</code></li> </ul>"},{"location":"processing-pipeline/","title":"Processing Pipeline","text":"<p>This page describes how the FPVS Toolbox processes EEG data from raw recordings to frequency-domain metrics and ROI-level summaries.</p> <p>The pipeline is designed to mirror a standard processing pipeline using MATLAB and EEGLab. Please see the relevant  publications for more information on this methodology. </p>"},{"location":"processing-pipeline/#overview","title":"Overview","text":"<p>At a high level, the FPVS Toolbox:</p> <ol> <li>Loads a raw FPVS recording (BioSemi <code>.bdf</code>) using    a disk-backed memory map.</li> <li>Applies a standardized preprocessing pipeline (referencing, filtering,    artifact handling, final average reference).</li> <li>Extracts events for user-defined conditions and creates epochs around    each stimulus type.</li> <li>Computes frequency-domain spectra and FPVS metrics (e.g., SNR, baseline-    corrected amplitudes) at the tag frequency and harmonics.</li> <li>Aggregates results within user-defined ROIs and exports per-condition,    per-ROI summaries to Excel for downstream statistics.</li> </ol> <p>Each step is logged to the FPVS Toolbox log window and log file so users can reconstruct the full processing history for a given project.</p>"},{"location":"processing-pipeline/#1-loading-recordings","title":"1. Loading recordings","text":"<p>When you select a single file or a batch folder, the loader:</p> <ul> <li> <p>Resolves the stimulus channel   The toolbox chooses the stim channel specified in project settings   (default: <code>Status</code>) and assigns it the MNE <code>stim</code> type.</p> </li> <li> <p>Resolves the initial reference pair   A pair of EXG channels (default: <code>EXG1</code> / <code>EXG2</code>) is treated as the   initial EEG reference. These channels are kept as EEG during loading.</p> </li> <li> <p>Reads the recording with MNE  </p> </li> <li>BioSemi recordings are loaded with <code>mne.io.read_raw_bdf</code>.</li> <li> <p>EEGLAB files are loaded via MNE\u2019s EEGLAB reader.   In both cases, a disk-backed memory map is used when possible to limit   RAM usage on large datasets.</p> </li> <li> <p>Applies a standard montage   After loading, a standard 10\u201320 montage is applied to EEG channels so   channel locations are available for interpolation and ROI aggregation.   Any non-EEG auxiliary channels are typed appropriately (e.g., EXG \u2192 <code>misc</code>   after referencing, stim \u2192 <code>stim</code>).</p> </li> </ul>"},{"location":"processing-pipeline/#2-preprocessing-pipeline","title":"2. Preprocessing pipeline","text":"<p>The preprocessing pipeline is applied in a fixed order to keep behavior consistent across projects.</p>"},{"location":"processing-pipeline/#21-initial-referencing","title":"2.1 Initial referencing","text":"<ul> <li>The user-selected reference pair (by default, <code>EXG1</code> / <code>EXG2</code>) is   coerced to EEG type if needed.</li> <li>MNE\u2019s <code>set_eeg_reference</code> is used to subtract the average of these two   reference channels from all EEG channels.</li> <li>Audit flags are recorded so that the log reflects whether referencing   succeeded and which channels were used.</li> </ul>"},{"location":"processing-pipeline/#22-drop-reference-channels","title":"2.2 Drop reference channels","text":"<p>After re-referencing:</p> <ul> <li>The two reference channels are removed from the dataset, so they do not   contribute to later average-reference and ROI computations.</li> </ul>"},{"location":"processing-pipeline/#23-optional-channel-limit","title":"2.3 Optional channel limit","text":"<p>If a maximum EEG channel count is configured in project settings:</p> <ul> <li>Channels beyond that index are dropped to keep a consistent subset of   sensors across recordings.</li> <li>The stim channel is preserved even if it would otherwise be outside   the limit.</li> </ul>"},{"location":"processing-pipeline/#24-downsampling-optional","title":"2.4 Downsampling (optional)","text":"<p>If downsampling is enabled and the original sampling rate is higher than the chosen target:</p> <ul> <li>Data are resampled using MNE\u2019s resampling routines with a Hann window.</li> <li>Downsampling occurs before filtering to reduce computation and   keep filter design consistent.</li> </ul>"},{"location":"processing-pipeline/#25-fir-filtering","title":"2.5 FIR filtering","text":"<p>A zero-phase FIR filter is applied to the (possibly downsampled) data:</p> <ul> <li>Legacy parameters are mirrored:</li> <li>The GUI \u201chigh-pass\u201d setting maps to the filter\u2019s <code>l_freq</code>.</li> <li>The GUI \u201clow-pass\u201d setting maps to <code>h_freq</code>.</li> <li>Fixed transition bandwidths and filter lengths are chosen to match     the original toolbox behavior as closely as possible.</li> <li>Filtering is applied in a way that avoids phase distortion   (forward-and-backward filtering).</li> </ul>"},{"location":"processing-pipeline/#26-kurtosis-based-artifact-handling","title":"2.6 Kurtosis-based artifact handling","text":"<p>To identify noisy channels:</p> <ul> <li>EEG channels (excluding previously marked bads and non-EEG/stim   channels) are scored using a kurtosis-based metric.</li> <li>Channels whose kurtosis exceeds a configurable Z-score threshold are:</li> <li>Marked as bad (added to <code>raw.info[\"bads\"]</code>).</li> <li>Interpolated when a montage is available, using neighboring     electrodes to estimate the signal.</li> </ul>"},{"location":"processing-pipeline/#27-final-average-reference","title":"2.7 Final average reference","text":"<p>After bad-channel handling:</p> <ul> <li>Remaining good EEG channels are re-referenced to the average.</li> <li>Any pending projections are applied.</li> <li>Preprocessing completes with logging of:</li> <li>Final channel count (total and bads).</li> <li>Final sampling rate.</li> <li>Filter settings, artifact thresholds, and reference choices.</li> </ul>"},{"location":"processing-pipeline/#3-event-extraction-and-epoching","title":"3. Event extraction and epoching","text":"<p>The event and epoching step mirrors the legacy flow but is configured via the PySide6 GUI.</p>"},{"location":"processing-pipeline/#31-event-detection","title":"3.1 Event detection","text":"<p>Events are derived in one of two ways:</p> <ol> <li> <p>From annotations    If the recording contains MNE annotations that correspond to your    condition IDs, they are mapped directly using the IDs and labels you    configure in the Event Map.</p> </li> <li> <p>From the stim channel    If annotations are not available, events are detected from the stim    channel (e.g., <code>Status</code>) using <code>mne.find_events</code>.  </p> </li> <li>The IDs you configure in the Event Map (e.g., <code>21</code> for      \u201cPositive Valence\u201d) are matched to codes on the stim channel.</li> <li>Empty event sets (no events found for a given ID) are logged as      warnings.</li> </ol>"},{"location":"processing-pipeline/#32-epoch-creation","title":"3.2 Epoch creation","text":"<p>For each label/ID pair defined in the Event Map:</p> <ul> <li>MNE <code>Epochs</code> objects are created with user-specified start and end   times relative to each event (e.g., from \u22120.5 s to +5.0 s).</li> <li>Epochs inherit the preprocessed, average-referenced data, so later   frequency-domain analysis operates on cleaned signals.</li> <li>Successful epoch sets are stored per condition label and used for   downstream spectral analysis and metric computation.</li> </ul>"},{"location":"processing-pipeline/#4-frequency-domain-analysis","title":"4. Frequency-domain analysis","text":"<p>Once epochs are defined, the FPVS Toolbox computes frequency-domain responses for each condition.</p> <p>In brief:</p> <ol> <li>FFT per epoch and channel </li> <li> <p>A Fourier transform is applied to each epoch to obtain amplitude      spectra at frequencies of interest.</p> </li> <li> <p>Tag frequency and harmonics </p> </li> <li> <p>The fundamental FPVS stimulation frequency and a configurable number      of harmonics are selected for metric computation.</p> </li> <li> <p>Baseline regions </p> </li> <li>Surrounding \u201cnoise\u201d frequency bins (excluding the signal bin and      its immediate neighbors) define a baseline used for SNR and      baseline-corrected measures.</li> </ol>"},{"location":"processing-pipeline/#5-fpvs-metrics-snr-baseline-corrected-amplitude","title":"5. FPVS metrics (SNR, baseline-corrected amplitude)","text":"<p>For each channel, condition, and harmonic, the toolbox computes: - SNR (Signal-to-Noise Ratio) </p> <p>In frequency-domain analyses of fast periodic visual stimulation (FPVS) with electroencephalography (EEG), signal-to-noise ratio (SNR) is a core metric for quantifying neural responses at stimulation frequencies. Within oddball paradigms\u2014where deviant stimuli are periodically interleaved among a base stream\u2014SNR provides a robust, objective estimate of response strength at the oddball frequency and its harmonics. The standard method for calculating SNR involves dividing the amplitude at the frequency of interest by the mean amplitude of surrounding frequency bins, assumed to reflect background noise (Rossion et al., 2015; Zimmermann et al., 2019). A methodological consensus has emerged around the use of a 20-bin local noise window\u201410 bins on either side of the target frequency\u2014with specific exclusions to ensure a clean estimate. First, the immediately adjacent frequency bins are systematically excluded to mitigate contamination from spectral leakage of the target response (Stothart et al., 2017; Georges et al., 2020). Second, many studies further discard the two most extreme amplitude values among the noise bins (i.e., the highest and lowest), which can otherwise inflate or deflate the average due to unrelated spectral artifacts or narrowband EEG noise (Dzhelyova &amp; Rossion, 2014a; Poncet et al., 2019). For example, Georges et al. (2020) and Zimmermann et al. (2019) both implemented this 20-bin \u00b110 approach, excluding the neighboring bins as well as the extreme values to derive a robust noise floor. This practice, introduced in foundational studies (e.g., Liu-Shuang et al., 2014), ensures the SNR reflects genuine periodic neural responses rather than local noise fluctuations or harmonics from unrelated sources. The resulting SNR values are used not only for individual participant-level inference but also for generating z-scores and evaluating signal strength across scalp topographies. Because the frequency bins used for noise estimation are drawn from the same power spectrum and close in frequency to the signal bin, this method yields a locally normalized, frequency-specific measure that is highly sensitive to subtle categorical effects in the EEG signal (Rossion et al., 2015; Poncet et al., 2019).</p> <p>References</p> <ul> <li>Dzhelyova, M., &amp; Rossion, B. (2014a). The effect of parametric stimulus variation on individual face discrimination indexed by fast periodic visual stimulation. Journal of Vision, 14(12), 1\u201318. https://doi.org/10.1167/14.12.22</li> <li>Georges, C., Retter, T. L., &amp; Rossion, B. (2020). Face-selective responses in the human brain: A periodic stimulation approach. NeuroImage, 214, 116703. https://doi.org/10.1016/j.neuroimage.2020.116703</li> <li>Liu-Shuang, J., Norcia, A. M., &amp; Rossion, B. (2014). An objective index of individual face discrimination in the right occipito-temporal cortex by means of fast periodic oddball stimulation. Neuropsychologia, 52, 57\u201372. https://doi.org/10.1016/j.neuropsychologia.2013.10.022</li> <li>Poncet, F., Rossion, B., &amp; Jacques, C. (2019). Evidence for the existence of a face-selective neural response in the human brain with fast periodic visual stimulation. NeuroImage, 189, 150\u2013162. https://doi.org/10.1016/j.neuroimage.2019.01.021</li> <li>Rossion, B., Torfs, K., Jacques, C., &amp; Liu-Shuang, J. (2015). Fast periodic presentation of natural images reveals a robust face-selective electrophysiological response in the human brain. Journal of Vision, 15(1), 1\u201318. https://doi.org/10.1167/15.1.15</li> <li>Stothart, G., Quadflieg, S., &amp; Kazanina, N. (2017). Semantic processing in the human brain: Electrophysiological evidence from fast periodic visual stimulation. Neuropsychologia, 100, 57\u201363. https://doi.org/10.1016/j.neuropsychologia.2017.03.006</li> <li> <p>Zimmermann, J. F., Groh-Bordin, C., &amp; Roesler, F. (2019). Familiarity and identity-specificity of face representations in fast periodic visual stimulation. NeuroImage, 193, 162\u2013171. https://doi.org/10.1016/j.neuroimage.2019.03.026</p> </li> <li> <p>Baseline-corrected amplitude (BCA)   Amplitude at the target bin minus the baseline estimate.</p> </li> </ul> <p>These metrics are then aggregated:</p> <ul> <li>Across epochs for each subject and condition.</li> <li>Across channels belonging to each user-defined ROI.</li> </ul> <p>The resulting ROI-level SNR and BCA values form the basis of the statistical analyses run in the Stats tool.</p>"},{"location":"processing-pipeline/#6-roi-aggregation-and-exports","title":"6. ROI aggregation and exports","text":"<p>The final steps of the processing pipeline are:</p> <ol> <li>ROI aggregation </li> <li>Channels are grouped into ROIs defined in project settings      (e.g., left occipital, right occipital, frontal, parietal).</li> <li> <p>Metrics are averaged within each ROI for each subject, condition,      and harmonic.</p> </li> <li> <p>Excel exports </p> </li> <li>Per-subject, per-condition, and per-ROI summaries are written to      Excel files in the project\u2019s Results folder.</li> <li> <p>These Excel outputs are used directly by the Statistical Analysis      module for single-group and between-group models.</p> </li> <li> <p>Logging and audit trail </p> </li> <li>Each successfully processed file contributes to a batch summary      (number of files, rejected channels, etc.).</li> <li>The logs provide a full audit trail of the processing settings used      for a given project and batch.</li> </ol>"},{"location":"processing-pipeline/#notes-and-best-practices","title":"Notes and best practices","text":"<ul> <li>Keep preprocessing settings consistent within a project so that metrics   are comparable across subjects and sessions.</li> <li>Verify event IDs and epoch time windows before running large batch   jobs; incorrect event mappings can silently produce empty conditions.</li> <li>When publishing results, include key pipeline settings (reference,   filter bands, artifact thresholds, and epoch windows) in the Methods   section, ideally referencing this documentation for additional detail.</li> </ul>"},{"location":"relevant-publications/","title":"Relevant Publications","text":"<p>This page lists key articles and books that informed the design of the FPVS Toolbox, including choices in paradigm design, preprocessing, and statistical analysis. It is not exhaustive, but it should give users a solid starting point to understand the methods behind the software.</p>"},{"location":"relevant-publications/#fpvs-ssvep-and-frequency-tagging-paradigms","title":"FPVS, SSVEP, and Frequency-Tagging Paradigms","text":"<ul> <li> <p>Vandenheever et. al, 2025 Exploring facial expression processing with fast periodic visual stimulation and diverse stimuli. Brain and Cognition. 2025; doi:10.1016/j.bandc.2025.106338.</p> </li> <li> <p>Vandenheever et. al, 2025. Preliminary evidence for anxiety linked neural sensitivity to emotional faces using fast periodic visual stimulation. International Journal of Psychophysiology. doi: 10.1016/j.ijpsycho.2025.113212.</p> </li> <li> <p>Rossion B, Torfs K, Jacques C, Liu-Shuang J. Fast periodic presentation of natural images reveals a robust face-selective electrophysiological response in the human brain. J Vis. 2015;15(1):15.1.18. Published 2015 Jan 16. doi:10.1167/15.1.18</p> </li> <li> <p>Rossion B, Retter TL, Liu-Shuang J. Understanding human individuation of unfamiliar faces with oddball fast periodic visual stimulation and electroencephalography. Eur J Neurosci. 2020;52(10):4283-4344. doi:10.1111/ejn.14865</p> </li> </ul> <p>These papers provide the main conceptual and analytical background for FPVS paradigms, oddball designs, and harmonic-based frequency-tagging analyses used in the Toolbox.</p>"},{"location":"relevant-publications/#eeg-erp-methods-and-preprocessing","title":"EEG / ERP Methods and Preprocessing","text":"<ul> <li> <p>Gil \u00c1vila, C., Bott, F.S., Tiemann, L. et al. DISCOVER-EEG: an open, fully automated EEG pipeline for biomarker discovery in clinical neuroscience. Sci Data 10, 613 (2023). https://doi.org/10.1038/s41597-023-02525-0</p> </li> <li> <p>Hern\u00e1ndez-Mustieles, M. A., Lima-Carmona, Y. E., Mendoza-Armenta, A. A., Hernandez-Machain, X., Garza-V\u00e9lez, D. A., Carrillo-M\u00e1rquez, A., Rodr\u00edguez-Alvarado, D. C., Lozoya-Santos, J. d. J., &amp; Ram\u00edrez-Moreno, M. A. (2024). An EEG Dataset of Subject Pairs during Collaboration and Competition Tasks in Face-to-Face and Online Modalities. Data, 9(4), 47. https://doi.org/10.3390/data9040047</p> </li> </ul> <p>These references give general foundations for EEG/ERP recording, artifact handling, filtering, referencing, and statistical analysis that inform the preprocessing pipeline.</p>"},{"location":"relevant-publications/#statistical-methods-and-linear-mixed-models","title":"Statistical Methods and Linear Mixed Models","text":"<ul> <li> <p>Boisgontier MP, Cheval B. The anova to mixed model transition. Neurosci Biobehav Rev. 2016;68:1004-1005. doi:10.1016/j.neubiorev.2016.05.034</p> </li> <li> <p>Matuschek, Hannes &amp; Kliegl, Reinhold &amp; Vasishth, Shravan &amp; Baayen, Harald &amp; Bates, Douglas. (2017). Balancing Type I Error and Power in Linear Mixed Models. Journal of Memory and Language. 94. 305\u2013315. 10.1016/j.jml.2017.01.001. </p> </li> <li> <p>Heise MJ, Mon SK, Bowman LC. Utility of linear mixed effects models for event-related potential research with infants and children. Dev Cogn Neurosci. 2022;54:101070. doi:10.1016/j.dcn.2022.101070</p> </li> </ul> <p>These works underpin the use of Benjamini\u2013Hochberg FDR correction and linear mixed-effects models for the statistical outputs generated by the FPVS Toolbox.</p>"},{"location":"relevant-publications/#how-to-cite-the-fpvs-toolbox","title":"How to Cite the FPVS Toolbox","text":"<p>If you use the FPVS Toolbox in a publication, please consider citing:</p> <p>Murphy Z., et al. FPVS Toolbox: Automated preprocessing and statistical analysis for fast periodic visual stimulation EEG experiments. [Manuscript in preparation / preprint details to be added here.]</p> <p>Update this section with the final reference once a preprint or paper is available.</p>"},{"location":"statistical-analysis/","title":"Statistical Analysis","text":"<p>This page summarizes the statistical models used by the FPVS Toolbox and how to interpret the outputs produced by the Statistical Analysis module.</p> <p>All analyses are run on preprocessed FPVS data after spectral analysis and ROI aggregation.</p>"},{"location":"statistical-analysis/#data-analyzed","title":"Data analyzed","text":"<ul> <li>The primary dependent variable is the summed baseline-corrected   oddball amplitude (Summed BCA).</li> <li>Analyses are performed at the level of:</li> <li>subject \u00d7 condition \u00d7 ROI.</li> </ul> <p>Unless otherwise noted, all sections below refer to Summed BCA.</p>"},{"location":"statistical-analysis/#single-group-analyses","title":"Single-group analyses","text":"<p>Single-group mode analyzes one experimental group at a time.</p>"},{"location":"statistical-analysis/#repeated-measures-anova-rm-anova","title":"Repeated-measures ANOVA (RM-ANOVA)","text":"<ul> <li>Within-subject factors:</li> <li>Condition</li> <li>ROI</li> <li>Implementation:</li> <li>When available, the toolbox uses Pingouin\u2019s <code>rm_anova</code>, reporting:<ul> <li>uncorrected p-values;</li> <li>Greenhouse\u2013Geisser (GG) and Huynh\u2013Feldt (HF) corrected p-values.</li> </ul> </li> <li>Otherwise, it falls back to statsmodels <code>AnovaRM</code>     (uncorrected p-values only).</li> <li>Effects of interest typically include:</li> <li>main effect of condition;</li> <li>main effect of ROI;</li> <li>condition \u00d7 ROI interaction.</li> </ul>"},{"location":"statistical-analysis/#post-hoc-tests","title":"Post-hoc tests","text":"<ul> <li>Pairwise paired t-tests between conditions.</li> <li>Tests are run separately within each ROI.</li> <li>Multiple-comparison control:</li> <li>p-values are corrected using the Benjamini\u2013Hochberg false discovery     rate (FDR) procedure.</li> <li>Exports include:<ul> <li>raw p-values;</li> <li>FDR-adjusted p-values (<code>p_fdr_bh</code>);</li> <li>effect sizes (e.g., Cohen\u2019s d).</li> </ul> </li> </ul>"},{"location":"statistical-analysis/#linear-mixed-effects-model-single-group","title":"Linear mixed-effects model (single group)","text":"<ul> <li>A linear mixed-effects model is fit to Summed BCA with:</li> <li>fixed effects: condition, ROI, and condition \u00d7 ROI     interaction;</li> <li>random effects: random intercept for each subject.</li> <li>No additional multiple-comparison correction is applied to individual   coefficients in this model; interpretation focuses on the pattern and   significance of fixed-effect terms.</li> </ul>"},{"location":"statistical-analysis/#multi-group-analyses","title":"Multi-group analyses","text":"<p>Multi-group mode (often referred to as \u201cLela mode\u201d) compares two or more groups (e.g., different hormonal status, clinical vs control).</p>"},{"location":"statistical-analysis/#between-group-anova-mixed-anova","title":"Between-group ANOVA (mixed ANOVA)","text":"<ul> <li>Factors:</li> <li>Group (between-subjects)</li> <li>Condition (within-subjects)</li> <li>Dependent variable:</li> <li>Summed BCA, typically collapsed across ROI for this test.</li> <li>Implementation:</li> <li>When available, the toolbox uses Pingouin\u2019s <code>mixed_anova</code>.</li> <li>Key effects:</li> <li>main effect of group;</li> <li>main effect of condition;</li> <li>group \u00d7 condition interaction.</li> </ul>"},{"location":"statistical-analysis/#between-group-mixed-model","title":"Between-group mixed model","text":"<ul> <li>A linear mixed-effects model is fit to Summed BCA with:</li> <li>fixed effects: group, condition, ROI, and their     interactions;</li> <li>random effects: random intercept per subject.</li> <li>This model allows testing whether the pattern of responses across   conditions and ROIs differs by group while accounting for repeated   measures within subjects.</li> </ul>"},{"location":"statistical-analysis/#group-contrasts","title":"Group contrasts","text":"<ul> <li>Pairwise group comparisons are computed separately for each   condition \u00d7 ROI combination.</li> <li>Implementation:</li> <li>Welch\u2019s t-tests (unequal variances) between groups.</li> <li>Multiple-comparison control:</li> <li>p-values are corrected with Benjamini\u2013Hochberg FDR.</li> <li>Exports include:<ul> <li>raw p-values;</li> <li>FDR-adjusted p-values;</li> <li>effect sizes (e.g., Cohen\u2019s d).</li> </ul> </li> </ul>"},{"location":"statistical-analysis/#output-files-and-interpretation","title":"Output files and interpretation","text":"<ul> <li>All model results are written to Excel in the project\u2019s <code>3 - Statistical Analysis Results</code> folder.</li> <li>Typical exports include:</li> <li>ANOVA tables (F, df, p, corrected p when applicable);</li> <li>mixed-model summaries (fixed-effect estimates, standard errors,     t-values, p-values);</li> <li>post-hoc and group-contrast tables with raw and FDR-adjusted p-values     and effect sizes.</li> </ul> <p>Consult these Excel tables when reporting results in manuscripts or presentations.</p>"},{"location":"statistical-analysis/#general-notes","title":"General notes","text":"<ul> <li>Unless otherwise noted, the default alpha level is 0.05.</li> <li>FDR correction reduces the chance of false positives when running many   post-hoc tests but does not guarantee that all significant findings   will replicate.</li> <li>Users remain responsible for:</li> <li>checking model assumptions (normality, sphericity, etc.);</li> <li>choosing effect sizes and contrasts that match their hypotheses;</li> <li>transparently reporting all relevant results in line with field     standards.</li> </ul>"},{"location":"tutorial/","title":"FPVS Toolbox: Getting Started","text":"<ol> <li>First, download the FPVS Toolbox installer from the latest release on github. Follow all instructions. You may have to manually bypass Windows Defender or other antivirus software.</li> </ol> <p>The rest of the FPVS Toolbox tutorial will be added soon. </p>"}]}